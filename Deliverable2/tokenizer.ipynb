{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZizltCjD8O7"
      },
      "source": [
        "#Step:1-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxyrZqs0rDJd"
      },
      "source": [
        "## Demonstration of text being converted to tokens.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-MelCvJqH61",
        "outputId": "d07de826-5c47-4555-9ace-5b599556f84f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "\n",
            "Artificial Intelligence (AI) is changing the world.\n",
            "‰∫∫Â∑•Áü•ËÉΩ„ÅØ‰∏ñÁïå„ÇíÂ§â„Åà„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n",
            "\n",
            "def greet(name):\n",
            "    print(f\"Hello, {name}!\")  # Greet the user\n",
            "\n",
            "# „Åì„Çì„Å´„Å°„ÅØ„ÄÅ„É¶„Éº„Ç∂„Éº„Åï„ÇìÔºÅ\n",
            "greet(\"‰∏ñÁïå\")\n",
            "\n",
            "Data analysis and natural language processing (NLP) are core parts of AI.\n",
            "„Éá„Éº„ÇøÂàÜÊûê„Å®Ë®ÄË™ûÂá¶ÁêÜ„ÅØAI„ÅÆ‰∏≠Ê†∏„Åß„Åô„ÄÇ\n",
            "\n",
            "length: 258\n",
            "---\n",
            "[10, 65, 114, 116, 105, 102, 105, 99, 105, 97, 108, 32, 73, 110, 116, 101, 108, 108, 105, 103, 101, 110, 99, 101, 32, 40, 65, 73, 41, 32, 105, 115, 32, 99, 104, 97, 110, 103, 105, 110, 103, 32, 116, 104, 101, 32, 119, 111, 114, 108, 100, 46, 10, 228, 186, 186, 229, 183, 165, 231, 159, 165, 232, 131, 189, 227, 129, 175, 228, 184, 150, 231, 149, 140, 227, 130, 146, 229, 164, 137, 227, 129, 136, 227, 129, 166, 227, 129, 132, 227, 129, 190, 227, 129, 153, 227, 128, 130, 10, 10, 100, 101, 102, 32, 103, 114, 101, 101, 116, 40, 110, 97, 109, 101, 41, 58, 10, 32, 32, 32, 32, 112, 114, 105, 110, 116, 40, 102, 34, 72, 101, 108, 108, 111, 44, 32, 123, 110, 97, 109, 101, 125, 33, 34, 41, 32, 32, 35, 32, 71, 114, 101, 101, 116, 32, 116, 104, 101, 32, 117, 115, 101, 114, 10, 10, 35, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 227, 128, 129, 227, 131, 166, 227, 131, 188, 227, 130, 182, 227, 131, 188, 227, 129, 149, 227, 130, 147, 239, 188, 129, 10, 103, 114, 101, 101, 116, 40, 34, 228, 184, 150, 231, 149, 140, 34, 41, 10, 10, 68, 97, 116, 97, 32, 97, 110, 97, 108, 121, 115, 105, 115, 32, 97, 110, 100, 32, 110, 97, 116, 117, 114, 97, 108, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 112, 114, 111, 99, 101, 115, 115, 105, 110, 103, 32, 40, 78, 76, 80, 41, 32, 97, 114, 101, 32, 99, 111, 114, 101, 32, 112, 97, 114, 116, 115, 32, 111, 102, 32, 65, 73, 46, 10, 227, 131, 135, 227, 131, 188, 227, 130, 191, 229, 136, 134, 230, 158, 144, 227, 129, 168, 232, 168, 128, 232, 170, 158, 229, 135, 166, 231, 144, 134, 227, 129, 175, 65, 73, 227, 129, 174, 228, 184, 173, 230, 160, 184, 227, 129, 167, 227, 129, 153, 227, 128, 130, 10]\n",
            "length: 352\n"
          ]
        }
      ],
      "source": [
        "test_string = '''\n",
        "Artificial Intelligence (AI) is changing the world.\n",
        "‰∫∫Â∑•Áü•ËÉΩ„ÅØ‰∏ñÁïå„ÇíÂ§â„Åà„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n",
        "\n",
        "def greet(name):\n",
        "    print(f\"Hello, {name}!\")  # Greet the user\n",
        "\n",
        "# „Åì„Çì„Å´„Å°„ÅØ„ÄÅ„É¶„Éº„Ç∂„Éº„Åï„ÇìÔºÅ\n",
        "greet(\"‰∏ñÁïå\")\n",
        "\n",
        "Data analysis and natural language processing (NLP) are core parts of AI.\n",
        "„Éá„Éº„ÇøÂàÜÊûê„Å®Ë®ÄË™ûÂá¶ÁêÜ„ÅØAI„ÅÆ‰∏≠Ê†∏„Åß„Åô„ÄÇ\n",
        "'''\n",
        "\n",
        "tokens = test_string.encode(\"utf-8\") # raw bytes\n",
        "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
        "print('---')\n",
        "print(test_string)\n",
        "print(\"length:\", len(test_string))\n",
        "print('---')\n",
        "print(tokens)\n",
        "print(\"length:\", len(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3OL4kibDteE"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FjaSN6Bu304"
      },
      "source": [
        "### The reason that encoding bytes are more (352>258) because it follows:\n",
        "\n",
        "UTF-8 is a **variable-length encoding**, meaning:\n",
        "\n",
        "- ASCII characters (e.g., `A-Z, 0-9, !@#`) ‚Üí **1 byte**\n",
        "- Latin and extended characters (e.g., `√©, √±, √º`) ‚Üí **2 bytes**\n",
        "- Non-Latin characters (e.g., `Êº¢Â≠ó, ‡§Ö, –£`) ‚Üí **3 or 4 bytes**\n",
        "- Emojis (e.g., `üòÑ, üá∫üá≥`) ‚Üí **4 bytes)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tGhV_AaEDGO"
      },
      "source": [
        "# Step 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-LyKgsnq_oz",
        "outputId": "f62e9267-85e6-41f1-cbc0-1edb1bfc9f51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(16, (227, 129)), (6, (101, 32)), (5, (227, 131)), (5, (227, 130)), (5, (114, 101)), (4, (110, 103)), (4, (110, 97)), (4, (97, 110)), (4, (32, 32)), (3, (228, 184)), (3, (227, 128)), (3, (188, 227)), (3, (131, 188)), (3, (129, 175)), (3, (116, 40)), (3, (115, 32)), (3, (105, 110)), (3, (101, 116)), (3, (101, 101)), (3, (97, 108)), (3, (65, 73)), (3, (41, 32)), (3, (32, 112)), (3, (32, 97)), (3, (10, 10)), (2, (231, 149)), (2, (184, 150)), (2, (166, 227)), (2, (153, 227)), (2, (150, 231)), (2, (149, 140)), (2, (147, 227)), (2, (130, 147)), (2, (130, 10)), (2, (129, 153)), (2, (128, 130)), (2, (116, 104)), (2, (115, 105)), (2, (114, 116)), (2, (112, 114)), (2, (111, 114)), (2, (110, 116)), (2, (109, 101)), (2, (108, 108)), (2, (108, 32)), (2, (105, 115)), (2, (104, 101)), (2, (103, 114)), (2, (103, 101)), (2, (103, 32)), (2, (102, 32)), (2, (101, 108)), (2, (99, 101)), (2, (97, 116)), (2, (97, 114)), (2, (97, 109)), (2, (46, 10)), (2, (35, 32)), (2, (34, 41)), (2, (32, 116)), (2, (32, 99)), (2, (32, 40)), (1, (239, 188)), (1, (232, 170)), (1, (232, 168)), (1, (232, 131)), (1, (231, 159)), (1, (231, 144)), (1, (230, 160)), (1, (230, 158)), (1, (229, 183)), (1, (229, 164)), (1, (229, 136)), (1, (229, 135)), (1, (228, 186)), (1, (191, 229)), (1, (190, 227)), (1, (189, 227)), (1, (188, 129)), (1, (186, 229)), (1, (186, 186)), (1, (184, 227)), (1, (184, 173)), (1, (183, 165)), (1, (182, 227)), (1, (175, 228)), (1, (175, 227)), (1, (175, 65)), (1, (174, 228)), (1, (173, 230)), (1, (171, 227)), (1, (170, 158)), (1, (168, 232)), (1, (168, 128)), (1, (167, 227)), (1, (166, 231)), (1, (165, 232)), (1, (165, 231)), (1, (164, 137)), (1, (161, 227)), (1, (160, 184)), (1, (159, 165)), (1, (158, 229)), (1, (158, 144)), (1, (149, 227)), (1, (147, 239)), (1, (146, 229)), (1, (144, 227)), (1, (144, 134)), (1, (140, 227)), (1, (140, 34)), (1, (137, 227)), (1, (136, 227)), (1, (136, 134)), (1, (135, 227)), (1, (135, 166)), (1, (134, 230)), (1, (134, 227)), (1, (132, 227)), (1, (131, 189)), (1, (131, 166)), (1, (131, 135)), (1, (130, 191)), (1, (130, 182)), (1, (130, 146)), (1, (129, 227)), (1, (129, 190)), (1, (129, 174)), (1, (129, 171)), (1, (129, 168)), (1, (129, 167)), (1, (129, 166)), (1, (129, 161)), (1, (129, 149)), (1, (129, 147)), (1, (129, 136)), (1, (129, 132)), (1, (129, 10)), (1, (128, 232)), (1, (128, 129)), (1, (125, 33)), (1, (123, 110)), (1, (121, 115)), (1, (119, 111)), (1, (117, 115)), (1, (117, 114)), (1, (117, 97)), (1, (116, 117)), (1, (116, 115)), (1, (116, 105)), (1, (116, 101)), (1, (116, 97)), (1, (116, 32)), (1, (115, 115)), (1, (115, 101)), (1, (114, 111)), (1, (114, 108)), (1, (114, 105)), (1, (114, 97)), (1, (114, 10)), (1, (112, 97)), (1, (111, 102)), (1, (111, 99)), (1, (111, 44)), (1, (110, 100)), (1, (110, 99)), (1, (108, 121)), (1, (108, 111)), (1, (108, 105)), (1, (108, 100)), (1, (108, 97)), (1, (105, 103)), (1, (105, 102)), (1, (105, 99)), (1, (105, 97)), (1, (104, 97)), (1, (103, 117)), (1, (103, 105)), (1, (102, 105)), (1, (102, 34)), (1, (101, 125)), (1, (101, 115)), (1, (101, 114)), (1, (101, 110)), (1, (101, 102)), (1, (101, 41)), (1, (100, 101)), (1, (100, 46)), (1, (100, 32)), (1, (99, 111)), (1, (99, 105)), (1, (99, 104)), (1, (97, 103)), (1, (97, 32)), (1, (80, 41)), (1, (78, 76)), (1, (76, 80)), (1, (73, 227)), (1, (73, 110)), (1, (73, 46)), (1, (73, 41)), (1, (72, 101)), (1, (71, 114)), (1, (68, 97)), (1, (65, 114)), (1, (58, 10)), (1, (44, 32)), (1, (41, 58)), (1, (41, 10)), (1, (40, 110)), (1, (40, 102)), (1, (40, 78)), (1, (40, 65)), (1, (40, 34)), (1, (34, 228)), (1, (34, 72)), (1, (33, 34)), (1, (32, 227)), (1, (32, 123)), (1, (32, 119)), (1, (32, 117)), (1, (32, 111)), (1, (32, 110)), (1, (32, 108)), (1, (32, 105)), (1, (32, 103)), (1, (32, 73)), (1, (32, 71)), (1, (32, 65)), (1, (32, 35)), (1, (10, 228)), (1, (10, 227)), (1, (10, 103)), (1, (10, 100)), (1, (10, 68)), (1, (10, 65)), (1, (10, 35)), (1, (10, 32))]\n"
          ]
        }
      ],
      "source": [
        "# Funtion to colab\n",
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]): # Pythonic way to iterate consecutive elements\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "stats = get_stats(tokens)\n",
        "print(sorted(((v,k) for k,v in stats.items()), reverse=True)) #This doing nothing but reversing the"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onZ1IcSkDz-C"
      },
      "source": [
        "## Step 3:\n",
        "What we are doing here:\n",
        "  1.Finding the pair of consequitive token.\n",
        "  2. Merging these consequitive token into a unicode token i furtherance of token dictionary so far formeed that means merging and assigning this pair to a new token ID in continues fashion ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk5KzDimDxx6"
      },
      "outputs": [],
      "source": [
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]): # Pythonic way to iterate consecutive elements\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "stats = get_stats(tokens)\n",
        "# print(stats)\n",
        "# print(sorted(((v,k) for k,v in stats.items()), reverse=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCAtR8sZE_h4",
        "outputId": "a3c73841-eae7-4554-d785-f8d88ae367fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(227, 129)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "top_pair = max(stats, key=stats.get)\n",
        "top_pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQVBOcybFNHi",
        "outputId": "c217ef81-ff6e-4b7e-a64a-800384700669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10, 65, 114, 116, 105, 102, 105, 99, 105, 97, 108, 32, 73, 110, 116, 101, 108, 108, 105, 103, 101, 110, 99, 101, 32, 40, 65, 73, 41, 32, 105, 115, 32, 99, 104, 97, 110, 103, 105, 110, 103, 32, 116, 104, 101, 32, 119, 111, 114, 108, 100, 46, 10, 228, 186, 186, 229, 183, 165, 231, 159, 165, 232, 131, 189, 256, 175, 228, 184, 150, 231, 149, 140, 227, 130, 146, 229, 164, 137, 256, 136, 256, 166, 256, 132, 256, 190, 256, 153, 227, 128, 130, 10, 10, 100, 101, 102, 32, 103, 114, 101, 101, 116, 40, 110, 97, 109, 101, 41, 58, 10, 32, 32, 32, 32, 112, 114, 105, 110, 116, 40, 102, 34, 72, 101, 108, 108, 111, 44, 32, 123, 110, 97, 109, 101, 125, 33, 34, 41, 32, 32, 35, 32, 71, 114, 101, 101, 116, 32, 116, 104, 101, 32, 117, 115, 101, 114, 10, 10, 35, 32, 256, 147, 227, 130, 147, 256, 171, 256, 161, 256, 175, 227, 128, 129, 227, 131, 166, 227, 131, 188, 227, 130, 182, 227, 131, 188, 256, 149, 227, 130, 147, 239, 188, 129, 10, 103, 114, 101, 101, 116, 40, 34, 228, 184, 150, 231, 149, 140, 34, 41, 10, 10, 68, 97, 116, 97, 32, 97, 110, 97, 108, 121, 115, 105, 115, 32, 97, 110, 100, 32, 110, 97, 116, 117, 114, 97, 108, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 112, 114, 111, 99, 101, 115, 115, 105, 110, 103, 32, 40, 78, 76, 80, 41, 32, 97, 114, 101, 32, 99, 111, 114, 101, 32, 112, 97, 114, 116, 115, 32, 111, 102, 32, 65, 73, 46, 10, 227, 131, 135, 227, 131, 188, 227, 130, 191, 229, 136, 134, 230, 158, 144, 256, 168, 232, 168, 128, 232, 170, 158, 229, 135, 166, 231, 144, 134, 256, 175, 65, 73, 256, 174, 228, 184, 173, 230, 160, 184, 256, 167, 256, 153, 227, 128, 130, 10]\n",
            "length: 336\n"
          ]
        }
      ],
      "source": [
        "def merge(ids, pair, idx):\n",
        "  # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
        "  newids = []\n",
        "  i = 0\n",
        "  while i < len(ids):\n",
        "    # if we are not at the very last position AND the pair matches, replace it\n",
        "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      newids.append(idx)\n",
        "      i += 2\n",
        "    else:\n",
        "      newids.append(ids[i])\n",
        "      i += 1\n",
        "  return newids\n",
        "\n",
        "tokens2 = merge(tokens, top_pair, 256) # Here we are making a fresh updated set of tokens .\n",
        "print(tokens2)\n",
        "print(\"length:\", len(tokens2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfyHAuLNE7zX"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyRAou55GBw6"
      },
      "source": [
        "# Step:4\n",
        "What we are doing is:\n",
        "  1. looping the merge\n",
        "  2. demonstrating each merge till no merge can follow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53vgyVWUF93D",
        "outputId": "b1b0a3be-4ef4-44bf-cee3-8bffa1482fff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "merging (227, 129) into a new token 256\n",
            "merging (101, 32) into a new token 257\n",
            "merging (227, 130) into a new token 258\n",
            "merging (227, 131) into a new token 259\n",
            "merging (97, 110) into a new token 260\n",
            "merging (32, 32) into a new token 261\n",
            "merging (97, 108) into a new token 262\n",
            "merging (65, 73) into a new token 263\n",
            "merging (115, 32) into a new token 264\n",
            "merging (105, 110) into a new token 265\n",
            "merging (256, 175) into a new token 266\n",
            "merging (228, 184) into a new token 267\n",
            "merging (227, 128) into a new token 268\n",
            "merging (10, 10) into a new token 269\n",
            "merging (114, 101) into a new token 270\n",
            "merging (270, 101) into a new token 271\n",
            "merging (271, 116) into a new token 272\n",
            "merging (110, 97) into a new token 273\n",
            "merging (259, 188) into a new token 274\n",
            "merging (114, 116) into a new token 275\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 276 # the desired final vocabulary size\n",
        "num_merges = vocab_size - 256\n",
        "ids = list(tokens) # copy so we don't destroy the original list\n",
        "\n",
        "merges = {} # (int, int) -> int\n",
        "for i in range(num_merges):\n",
        "  stats = get_stats(ids)\n",
        "  pair = max(stats, key=stats.get)\n",
        "  idx = 256 + i\n",
        "  print(f\"merging {pair} into a new token {idx}\")\n",
        "  ids = merge(ids, pair, idx)\n",
        "  merges[pair] = idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq-CKBkfHOKB"
      },
      "source": [
        "# Step 5:\n",
        "  1. Finaly we are printing number of token.\n",
        "  2. printing what was the lenght before merging.\n",
        "  3. A Compression Factor which is comprassion ratio which will bsically help us judge our merging process was usefull till which level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiuftSHMGiIG",
        "outputId": "f96bc326-d328-49c2-bc78-e5b7108a77e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens length: 352\n",
            "ids length: 272\n",
            "compression ratio: 1.29X\n"
          ]
        }
      ],
      "source": [
        "print(\"tokens length:\", len(tokens))\n",
        "print(\"ids length:\", len(ids))\n",
        "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1CfKfNMNEHA"
      },
      "source": [
        "# Building the whole cycle of encoding and decoding ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1ULOkMuKX3n"
      },
      "source": [
        "## Designing our own Encoder funtion which going to do multiple murges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5g6hRiIKJSq",
        "outputId": "08a11df5-05bb-4cab-db34-6d89369678d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10, 65, 275, 105, 102, 105, 99, 105, 262, 32, 73, 110, 116, 101, 108, 108, 105, 103, 101, 110, 99, 257, 40, 263, 41, 32, 105, 264, 99, 104, 260, 103, 265, 103, 32, 116, 104, 257, 119, 111, 114, 108, 100, 46, 10, 228, 186, 186, 229, 183, 165, 231, 159, 165, 232, 131, 189, 266, 267, 150, 231, 149, 140, 258, 146, 229, 164, 137, 256, 136, 256, 166, 256, 132, 256, 190, 256, 153, 268, 130, 269, 100, 101, 102, 32, 103, 272, 40, 273, 109, 101, 41, 58, 10, 261, 261, 112, 114, 265, 116, 40, 102, 34, 72, 101, 108, 108, 111, 44, 32, 123, 273, 109, 101, 125, 33, 34, 41, 261, 35, 32, 71, 272, 32, 116, 104, 257, 117, 115, 101, 114, 269, 35, 32, 256, 147, 258, 147, 256, 171, 256, 161, 266, 268, 129, 259, 166, 274, 258, 182, 274, 256, 149, 258, 147, 239, 188, 129, 10, 103, 272, 40, 34, 267, 150, 231, 149, 140, 34, 41, 269, 68, 97, 116, 97, 32, 260, 262, 121, 115, 105, 264, 260, 100, 32, 273, 116, 117, 114, 262, 32, 108, 260, 103, 117, 97, 103, 257, 112, 114, 111, 99, 101, 115, 115, 265, 103, 32, 40, 78, 76, 80, 41, 32, 97, 114, 257, 99, 111, 114, 257, 112, 97, 275, 264, 111, 102, 32, 263, 46, 10, 259, 135, 274, 258, 191, 229, 136, 134, 230, 158, 144, 256, 168, 232, 168, 128, 232, 170, 158, 229, 135, 166, 231, 144, 134, 266, 263, 256, 174, 267, 173, 230, 160, 184, 256, 167, 256, 153, 268, 130, 10]\n"
          ]
        }
      ],
      "source": [
        "def encode(text):\n",
        "  # given a string, return list of integers (the tokens)\n",
        "  tokens = list(text.encode(\"utf-8\"))\n",
        "  while len(tokens) >= 2:\n",
        "    stats = get_stats(tokens)\n",
        "    pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
        "    if pair not in merges:\n",
        "      break # nothing else can be merged\n",
        "    idx = merges[pair]\n",
        "    tokens = merge(tokens, pair, idx)\n",
        "  return tokens\n",
        "\n",
        "print(encode(test_string))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFl6A5wOKs00",
        "outputId": "854aa716-f15c-46ab-fb7f-6f9dcaa4e701"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "272\n"
          ]
        }
      ],
      "source": [
        "size=encode(test_string)\n",
        "print(len(size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKtDXjx-K96S",
        "outputId": "e04319a2-cf31-4b6f-d289-2b2f2fc0b2b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens length: 352\n",
            "ids length: 272\n",
            "compression ratio: 1.29X\n"
          ]
        }
      ],
      "source": [
        "print(\"tokens length:\", len(tokens))\n",
        "print(\"ids length:\", len(ids))\n",
        "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYCJAO5PL5x5"
      },
      "source": [
        "## Building Decoder to get back the original sentance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBQ0vcxAMbm1"
      },
      "outputs": [],
      "source": [
        "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "for (p0, p1), idx in merges.items():\n",
        "    vocab[idx] = vocab[p0] + vocab[p1]\n",
        "\n",
        "def decode(ids):\n",
        "  # given ids (list of integers), return Python string\n",
        "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
        "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohxCt-4oL5Xf",
        "outputId": "0ad1a7e1-07b1-481a-a782-2eee9815c9e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Artificial Intelligence (AI) is changing the world.\n",
            "‰∫∫Â∑•Áü•ËÉΩ„ÅØ‰∏ñÁïå„ÇíÂ§â„Åà„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n",
            "\n",
            "def greet(name):\n",
            "    print(f\"Hello, {name}!\")  # Greet the user\n",
            "\n",
            "# „Åì„Çì„Å´„Å°„ÅØ„ÄÅ„É¶„Éº„Ç∂„Éº„Åï„ÇìÔºÅ\n",
            "greet(\"‰∏ñÁïå\")\n",
            "\n",
            "Data analysis and natural language processing (NLP) are core parts of AI.\n",
            "„Éá„Éº„ÇøÂàÜÊûê„Å®Ë®ÄË™ûÂá¶ÁêÜ„ÅØAI„ÅÆ‰∏≠Ê†∏„Åß„Åô„ÄÇ\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(decode(encode(test_string)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
