{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7928df3",
   "metadata": {},
   "source": [
    "# Mental Health Assessment Tool - Project Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e943da6",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This project builds a Mental Health Assessment Assistant using Retrieval-Augmented Generation (RAG) techniques. The assistant aims to:\n",
    "\n",
    "- Provide relevant mental health advice.\n",
    "\n",
    "- Retrieve verified information from a curated dataset.\n",
    "\n",
    "- Generate responses using a lightweight LLM (Llama 3) via API.\n",
    "\n",
    "The primary focus is efficient local vector search combined with remote model inference.\n",
    "\n",
    "## 2. Dataset\n",
    "\n",
    "The data sources included multiple CSV files related to mental health forums, assessments, and therapy discussions. These were merged into a single large file:\n",
    "\n",
    "- Merged dataset: dataset/merged_posts.txt\n",
    "\n",
    "- Content: User posts, Q&A, therapy discussions, mental health experiences.\n",
    "\n",
    "The dataset was cleaned and prepared for text chunking.\n",
    "\n",
    "## 3. Vectorstore Building\n",
    "\n",
    "We created a persistent vectorstore using ChromaDB and SentenceTransformer (all-MiniLM-L6-v2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab2fde",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Load model with GPU acceleration\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda\")\n",
    "\n",
    "# Connect to ChromaDB persistent storage\n",
    "chroma_client = chromadb.PersistentClient(path=\"vectorstore\")\n",
    "collection = chroma_client.get_or_create_collection(\"mental_health\")\n",
    "\n",
    "# Chunk dataset and embed using batch processing\n",
    "# Store embeddings and text chunks into the ChromaDB collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95875ecd",
   "metadata": {},
   "source": [
    "## 4. Retrieval Mechanism\n",
    "\n",
    "For every user query, we:\n",
    "\n",
    "- Encode the query.\n",
    "\n",
    "- Perform vector similarity search in ChromaDB.\n",
    "\n",
    "- Retrieve the top K=3 most relevant chunks.\n",
    "\n",
    "This ensures the model gets rich, specific context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bfcdc4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_similar_chunks(query, top_k=3):\n",
    "    query_embedding = model.encode([query]).tolist()\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\"]\n",
    "    )\n",
    "    return results[\"documents\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95545ab6",
   "metadata": {},
   "source": [
    "## 5. LLM Integration\n",
    "\n",
    "Initially, we used a locally running Llama 3 model, but due to resource constraints, we switched to a remote API-based model hosting (like Together.ai or Ollama local server)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b312e9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_llama_response(prompt, context=\"\"):\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\n",
    "            \"model\": \"llama3\",\n",
    "            \"prompt\": f\"Context: {context}\\n\\nQuestion: {prompt}\\nAnswer:\",\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    return response.json()[\"response\"].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c21e1d",
   "metadata": {},
   "source": [
    "## 6. Chatbot Structure\n",
    "\n",
    "We combined retrieval and generation into a simple chatbot loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b0034",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from vectorstore import retrieve_similar_chunks\n",
    "from llm_integration import get_llama_response\n",
    "\n",
    "def chat():\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == 'quit':\n",
    "            break\n",
    "        context_chunks = retrieve_similar_chunks(query)\n",
    "        context = \"\\n\".join(context_chunks)\n",
    "        response = get_llama_response(prompt=query, context=context)\n",
    "        print(f\"\\nAssistant: {response}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cecf27c",
   "metadata": {},
   "source": [
    "This system is lightweight, modular, and easy to expand.\n",
    "\n",
    "## 7. Deployment Info\n",
    "\n",
    "- Vectorstore is stored in the vectorstore/ folder.\n",
    "\n",
    "- LLM API must be active at http://localhost:11434.\n",
    "\n",
    "- Simply run:\n",
    "\n",
    "    python chatbot.py\n",
    "\n",
    "- Chatbot will start interacting based on RAG outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef45fff",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 8. Future Scope\n",
    "\n",
    "### Short-Term Goals\n",
    "\n",
    "- Conversational Memory\n",
    "  \n",
    "- Mood Tracing\n",
    "\n",
    "- Add timers to restrict chatbot session activity.\n",
    "\n",
    "- Implement dynamic multi-turn conversations.\n",
    "\n",
    "- Design a user mental health assessment flow (objective + subjective).\n",
    "\n",
    "- Build a streamlined frontend (already under planning).\n",
    "\n",
    "### Long-Term Enhancements\n",
    "\n",
    "- Personalize responses based on user profiles.\n",
    "\n",
    "- Provide therapy-style guidance using dataset knowledge.\n",
    "\n",
    "- Integrate with larger LLMs or finetuned domain models.\n",
    "\n",
    "- Host the backend (API + vectorstore) on cloud for scalability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b8da22",
   "metadata": {},
   "source": [
    "## Links\n",
    "\n",
    "GitHub Repository - https://github.com/shutupatul/manasooth-bot"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
